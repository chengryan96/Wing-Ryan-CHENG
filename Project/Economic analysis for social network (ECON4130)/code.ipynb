{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup selemium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up selenium to use chrome\n",
    "options = Options()\n",
    "#options.add_argument(\"--headless\")\n",
    " #No need to open a browser window\n",
    "driver = webdriver.Chrome(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse\\chromedriver', options = options)\n",
    "\n",
    "# Example of manaully specifying the WebDriver's location: \n",
    "# driver = webdriver.Firefox(executable_path=\"../Others/geckodriver.exe\",options=options) #Windows\n",
    "# driver = webdriver.Firefox(executable_path=\"../Others/geckodriver\",options=options) #Linux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is to get the sibling tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling(tag,previous=False):\n",
    "    if previous:\n",
    "        sibling = tag.previous_sibling\n",
    "        while isinstance(sibling, NavigableString):\n",
    "            sibling = sibling.previous_sibling\n",
    "    else:\n",
    "        sibling = tag.next_sibling\n",
    "        while isinstance(sibling, NavigableString):\n",
    "            sibling = sibling.next_sibling        \n",
    "    return sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to scrape the place, gear and comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detail(url):\n",
    "        # Function to access a page and save all horses into a list\n",
    "\n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Is there anything?\n",
    "    if driver.page_source.find(\"No information.\") != -1:\n",
    "        return []\n",
    "    \n",
    "    # Wait 30 secs so that the dynamic content has time to load.\n",
    "    # Proceed to next date if page doesn't load.\n",
    " \n",
    "    \n",
    "    # Load the page into BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    # Find all tags with href containing \"horseno\"\n",
    "    detail = soup.find_all(\"table\",class_=\"table_bd\")\n",
    "    detail[0].get_text()\n",
    "    re.findall('\\\\n(B)', str(detail[0].text.strip()))\n",
    "    i=0\n",
    "    table = soup.findChildren('table', class_=\"table_bd\")[0]\n",
    "    place = re.findall(r'<td style=\\\"text-align: center;\\\">(\\d+)</td>\\n<', str(table))\n",
    "    gear = re.findall(r'\\\"width:36px; text-align: center;\\\">(.+)<', str(table))\n",
    "    comment = [re.findall(r'<td class=\\\"f_fs16 f_tal\\\">\\s+([A-z].+)', str(soup.find_all(\"td\",class_=\"f_fs16\")[i]))[0] for i in range(0, len(soup.find_all(\"td\",class_=\"f_fs16\")))]\n",
    "    lis = [[place[i], gear[i], comment[i]] for i in range(0, len(place))]\n",
    "    detail_df = pd.DataFrame.from_records(lis, columns = ['Placing', 'Gear', 'Comment'])\n",
    "    \n",
    "    return detail_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is to scrape the sector time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_sec_time(url):\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    stime1 = re.findall(r'<p>\\d{2}\\.\\d{2}|/racing/content/images/StaticFile/blank_img.gif', str(soup))\n",
    "    stime2 = [''.join(re.findall(r'\\d{2}\\.\\d{2}|/racing/content/images/StaticFile/blank_img.gif', stime1[i])) for i in range(0, len(stime1))]\n",
    "    stime3 = [''.join(re.sub(r'/racing/content/images/StaticFile/blank_img.gif', 'NA', stime2[i])) for i in range(0, len(stime1))]\n",
    "    time_list = [stime3[x:x+6] for x in range(0, len(stime3),6)]\n",
    "    time_df = pd.DataFrame.from_records(time_list, columns = ['1st Sec.', '2st Sec.', '3st Sec.', '4st Sec.', '5st Sec.', '6st Sec.'])\n",
    "    return time_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is to scrape the table from the hkjc website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_horses(url):\n",
    "    # Function to access a page and save all horses into a list\n",
    "\n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Is there anything?\n",
    "    if driver.page_source.find(\"No information.\") != -1:\n",
    "        return []\n",
    "    \n",
    "    # Wait 30 secs so that the dynamic content has time to load.\n",
    "    # Proceed to next date if page doesn't load.\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 30).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"f_fs13\")))\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    # Load the page into BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Find all tags with href containing \"HorseId\"\n",
    "    horses = soup.find_all(href=re.compile(\"HorseId\"))\n",
    "    \n",
    "    #find all tags with race number\n",
    "    race_no_soup = soup.find_all(\"tr\",class_=\"bg_blue color_w font_wb\")[0]\n",
    "    \n",
    "    \n",
    "    #find all tags with race info\n",
    "    race_info = soup.find_all(\"tbody\",class_=\"f_fs13\")[0]\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    # 'output_list' is the whole table\n",
    "    # 'output' is a single row\n",
    "    output_list = [['horse no', 'Horse', 'Jockey', 'Trainer', 'Actual Wt.', 'Declar. Horse Wt.', 'Draw', 'LBW', 'Running Position 1st sec', 'Running Position 2st sec', 'Running Position 3st sec', 'Running Position 4st sec', 'Running Position 5st sec', 'Running Position 6st sec' ,'Running Position', 'Finish Time', 'Win Odds', 'race no.','Class', 'length', 'grade', 'Going', 'com_name', 'course', 'prize']]\n",
    "    count = 0\n",
    "    # Loop through horses\n",
    "    for horse in horses:\n",
    "        \n",
    "        #race info\n",
    "        race_no = re.findall(r'RACE\\s\\d\\s\\((\\d+)', race_no_soup.text.strip())[0]\n",
    "        race_info_list = race_info.text.strip()\n",
    "        class_type = re.findall(r'(Class\\s\\d).{3}(\\d+M).{4}(\\d+-\\d+)', race_info_list)[0][0]\n",
    "        length = re.findall(r'(Class\\s\\d).{3}(\\d+M).{4}(\\d+-\\d+)', race_info_list)[0][1]\n",
    "        grade = re.findall(r'(Class\\s\\d).{3}(\\d+M).{4}(\\d+-\\d+)', race_info_list)[0][2]\n",
    "        going = re.findall(r'Going :\\n(.*)', race_info_list)[0]\n",
    "        com_name = re.findall(r'([A-Z].*)\\nCourse', race_info_list)[0]\n",
    "        course = re.findall(r'Course :\\n(.*)', race_info_list)[0]\n",
    "        prize = re.findall(r'HK\\$\\s(.*)', race_info_list)[0]\n",
    "        raceinfo = [class_type, length, grade, going, com_name, course, prize]\n",
    "        # Get the horse name\n",
    "        output = []        \n",
    "        \n",
    "        \n",
    "        # This while loop fetch all remaining fields in a row\n",
    "        a = get_sibling(horse.parent, previous = True)\n",
    "        while a != None:\n",
    "            output.append(a.text\n",
    "                          .strip()\n",
    "                          # The last two lines are for running positions\n",
    "                          .replace('\\n','') \n",
    "                          .replace(' '*20,' ') \n",
    "                         )\n",
    "            a = get_sibling(a)\n",
    "        \n",
    "        \n",
    "                          \n",
    "        #append other info to the output\n",
    "        [output.append(race_no)]\n",
    "        [output.append(i) for i in raceinfo]        \n",
    "        # Append each row to the output list\n",
    "        output_list.append(output) \n",
    "        count += 1\n",
    "        \n",
    "    for i in range(1, len(output_list)):\n",
    "        run_pos = output_list[i][8].split('   ')\n",
    "        while len(run_pos) < 6:\n",
    "            run_pos.extend('N')\n",
    "        output_list[i][8:8] = run_pos\n",
    "        output_df = pd.DataFrame(output_list[1:len(output_list)], columns = output_list[0])\n",
    "         \n",
    "\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is to scrape the odd of place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_place_odd(url):\n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Is there anything?\n",
    "    if driver.page_source.find(\"No information.\") != -1:\n",
    "        return []\n",
    "    \n",
    "    # Wait 30 secs so that the dynamic content has time to load.\n",
    "    # Proceed to next date if page doesn't load.\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    dividend = []\n",
    "    dividend_info = soup.find_all(\"td\",class_=\"f_fs14\")\n",
    "    for d in dividend_info:             #leave with white space\n",
    "        dividend.append(d.text.strip())\n",
    "    win_comb = [dividend[2*i] +'//' + dividend[2*i+1] for i in range(0, int(len(dividend)/2))]    \n",
    "    place = win_comb[1:4]\n",
    "    place_list = [i.split('//') for i in place]\n",
    "    for j in place_list:\n",
    "        j[1] = float(j[1])/10        \n",
    "    place_df = pd.DataFrame(place_list, columns = ['horse no', 'place odd'])\n",
    "    return place_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is to scrape the previous rtg and the previous race number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_previous_game(url):\n",
    "    # Function to access a page and save all horses into a list\n",
    "\n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Is there anything?\n",
    "    if driver.page_source.find(\"No information.\") != -1:\n",
    "        return []\n",
    "    \n",
    "    # Wait 30 secs so that the dynamic content has time to load.\n",
    "    # Proceed to next date if page doesn't load.\n",
    "    # Load the page into BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    race_no = re.findall(r'(\\d{3})<\\/a>', str(soup))\n",
    "    pre_race = race_no[1:len(race_no)]\n",
    "    pre_race.append('NA')\n",
    "    #rtg = re.findall(r't\">(\\d{2})<\\/td>\\n<td align=\"center\" class=\"htable_text\">', str(soup))\n",
    "    horse_name = soup.find_all(\"span\",class_=\"title_text\")[0].text.strip()\n",
    "    horse_name = re.findall(r'(.*)\\s(\\(.*\\d\\))', horse_name)\n",
    "    horse_name = ''.join(horse_name[0])\n",
    "    combine = [[horse_name, race_no[i], pre_race[i]]  for i in range(0, len(race_no))]\n",
    "    previous_df = pd.DataFrame(combine, columns = ['Horse', 'current race', 'previous race'])\n",
    "    return previous_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this loop is for the function scrape_horses and scrape_place_odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501011\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501012\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501013\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501014\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501015\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501016\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501017\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501018\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501041\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501042\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501043\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501044\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501045\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501046\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501047\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501048\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501049\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501071\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501073\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501074\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501075\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501077\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501078\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501101\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501102\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501103\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501104\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501105\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501106\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501107\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501108\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501109\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501141\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501143\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501144\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501146\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501147\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501148\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501181\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501182\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501183\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501184\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501185\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501186\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501187\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501189\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501211\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501213\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501214\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501215\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501216\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501217\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501218\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501251\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501252\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501253\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501254\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501255\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501256\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501259\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501281\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501282\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501283\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501284\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501285\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501286\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501287\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201501288\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502011\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502012\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502013\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502014\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502015\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502016\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502017\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502018\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502019\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502041\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502043\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502044\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502045\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502046\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502047\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502048\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502071\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502072\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502073\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502074\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502075\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502077\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502079\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502111\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502112\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502113\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502115\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502116\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502117\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502118\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502151\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502152\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502153\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502154\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502155\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502156\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502211\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502212\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502213\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502214\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502215\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502217\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502219\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502251\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502252\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502253\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502254\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502255\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502256\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502257\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201502259\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503011\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503012\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503013\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503014\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503016\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503019\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503041\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503043\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503044\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503045\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503046\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503047\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503048\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503049\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503081\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503082\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503084\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503085\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503086\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503087\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503088\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503111\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503112\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503114\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503115\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503117\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503118\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503119\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503151\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503152\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503153\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503154\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503155\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503156\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503158\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503181\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503182\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503183\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503184\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503185\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503186\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503187\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503188\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503211\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503212\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503213\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503214\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503215\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503216\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503217\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503218\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503219\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503251\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503252\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503253\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503254\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503255\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503256\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503257\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503258\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503291\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503292\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503293\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503294\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503295\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503296\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503297\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201503298\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504011\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504012\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504013\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504014\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504015\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504016\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504017\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504018\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504019\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504071\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504072\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504073\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504074\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504075\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504076\n",
      "http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/201504078\n"
     ]
    }
   ],
   "source": [
    "#URL of data\n",
    "url_front = \"http://racing.hkjc.com/racing/Info/Meeting/Results/english/Local/\"\n",
    "url_front_1 = 'https://racing.hkjc.com/racing/information/english/Reports/CORunning.aspx?Date='\n",
    "url_front_2 = '&RaceNo='\n",
    "url_front_3 = r'https://racing.hkjc.com/racing/information/english/Racing/DisplaySectionalTime.aspx?RaceDate='\n",
    "url_front_4 = r'&RaceNo='\n",
    "horse_full_df = pd.DataFrame()\n",
    "#Write a loop to go through year, month and day\n",
    "#Note that month and day is always 2 digit\n",
    "#Call scrape_horses() in each iteration\n",
    "for year in range(2015,2021):\n",
    "    for month in range(1,13):\n",
    "        for day in range(1,32):\n",
    "            for page in range(1,12):\n",
    "                try:\n",
    "                                        #Convert month and day to 2-digit representation\n",
    "                    month_2d = '{:02d}'.format(month)\n",
    "                    day_2d = '{:02d}'.format(day)\n",
    "                    page_1d = '{:01d}'.format(page)\n",
    "                    url = url_front + str(year) + month_2d + day_2d + page_1d\n",
    "                    url1 = url_front_1 + str(year) + month_2d + day_2d + url_front_2 + page_1d\n",
    "                    url2 = url_front_3  + day_2d + '/' + month_2d + '/' + str(year)+ url_front_4 + page_1d                \n",
    "                    horse_df = scrape_horses(url)                \n",
    "                    place_odd_df = scrape_place_odd(url)\n",
    "                    detail_df = scrape_detail(url1)\n",
    "                    sec_time_df = scrape_sec_time(url2)\n",
    "                    #concat the dataframe\n",
    "                    horse_table_df =  pd.concat([horse_df, place_odd_df, sec_time_df, detail_df], axis = 1)\n",
    "                    horse_full_df = horse_full_df.append(horse_table_df)\n",
    "                    print(url)           \n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "horse_full_df.to_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_full_df.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "read csv to loop through all horse detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_blood_df = pd.read_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse\\horse_info.csv')\n",
    "horse_name_list = horse_blood_df['Horse_name']\n",
    "horse_code = [''.join(re.findall(r'([A-Z]\\d{3})', name)) for name in horse_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_front_5 = \"https://racing.hkjc.com/racing/information/english/Horse/Horse.aspx?HorseNo=\"\n",
    "horse_previos_df = pd.DataFrame()\n",
    "for code in horse_code:\n",
    "    try:\n",
    "        url4 = url_front_5 + code\n",
    "        previous_df = scrape_previous_game(url4)\n",
    "        horse_previos_df = horse_previos_df.append(previous_df)\n",
    "        print(url4)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_previos_df.to_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_previous_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "#merge column\n",
    "horse_table_df = pd.read_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_full_df.csv')\n",
    "horse_previous_df = pd.read_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_previous_df.csv')\n",
    "horse_info = pd.read_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse\\horse_info.csv')\n",
    "horse_table_df = horse_table_df.iloc[:,1:len(horse_table_df.columns)]\n",
    "horse_previous_df.rename(columns={'current race':'race no.'}, inplace = True)\n",
    "horse_previous_df = horse_previous_df.iloc[:,1:len(horse_previous_df.columns)]\n",
    "\n",
    "table_prerace = pd.merge(horse_table_df, horse_previous_df, on=['race no.', 'Horse'], how='inner')\n",
    "\n",
    "#identify the 'curent' data\n",
    "new = table_prerace[['horse no', 'Horse', 'Jockey', 'Trainer', 'Class', 'Actual Wt.', 'Declar. Horse Wt.', 'grade', 'Going', 'com_name', 'Gear','course','previous race', 'Win Odds', 'place odd', 'race no.', 'Placing']]\n",
    "\n",
    "for col in new.columns: \n",
    "    new.rename(columns={col:('new_' + col)}, inplace = True)\n",
    "\n",
    "#rename previous race  \n",
    "table_prerace.rename(columns={'race no.':'previous race', 'previous race':'new_previous race'}, inplace = True)\n",
    "new.rename(columns={'new_previous race':'previous race', 'new_Horse':'Horse'}, inplace = True)\n",
    "\n",
    "#merge the new data and the previous data\n",
    "new_old = pd.merge(new, table_prerace, on=['previous race', 'Horse'], how='inner')\n",
    "\n",
    "import csv\n",
    "#new_old.to_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\new_old.csv')\n",
    "#\n",
    "new_old['new_Declar. Horse Wt.'] = pd.to_numeric(new_old['new_Declar. Horse Wt.'], errors='coerce')\n",
    "new_old['Declar. Horse Wt.'] = pd.to_numeric(new_old['Declar. Horse Wt.'], errors='coerce')\n",
    "\n",
    "#feature engineering\n",
    "#find the weight and actual weight difference\n",
    "weight_diff = new_old['new_Actual Wt.'] - new_old['Actual Wt.']\n",
    "declare_wt_diff = new_old['new_Declar. Horse Wt.'] - new_old['Declar. Horse Wt.']\n",
    "new_old['weight_diff'] = weight_diff\n",
    "new_old['declare_wt_diff'] = declare_wt_diff\n",
    "#merge horse blood\n",
    "#change the horse name\n",
    "def change_name(name):\n",
    "    return ''.join(re.findall(r'(.+)\\s(\\(.+\\))', name)[0])\n",
    "horse_info['Horse_name'] = horse_info['Horse_name'].apply(change_name)\n",
    "horse_info.rename(columns={'Horse_name':'Horse'}, inplace = True)\n",
    "\n",
    "#\n",
    "new_old['Horse']\n",
    "horse_df = pd.merge(horse_info, new_old, on = ['Horse', 'Trainer'], how='inner')\n",
    "#clean all --- to nan\n",
    "horse_df = horse_df.replace('---', '')\n",
    "horse_df = horse_df.replace('N', '')\n",
    "horse_df.to_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import require package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\Completed')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "# load libraries\n",
    "import numpy\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from bayes_opt import BayesianOptimization\n",
    "from matplotlib import pyplot\n",
    "pyplot.style.use('ggplot')          \n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from  sklearn.datasets  import  make_hastie_10_2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import contextlib\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data and one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_df = pd.read_csv(r'C:\\Users\\admin\\Desktop\\hw\\econ4130\\horse_df.csv')\n",
    "horse_df = horse_df.iloc[:,1:len(horse_df.columns)]\n",
    "a = horse_df.dtypes\n",
    "\n",
    "refer_index = []\n",
    "horse_df['new_Placing'] = horse_df['new_Placing'].fillna(0)\n",
    "c = horse_df['new_Placing']\n",
    "np.unique(c)\n",
    "horse_df['new_Placing'].dtype\n",
    "\n",
    "#one hot encoding\n",
    "for col in horse_df.columns:\n",
    "    try:\n",
    "        if horse_df.dtypes[col] != 'float64':\n",
    "            horse_df[col] = pd.factorize(horse_df[col])[0]\n",
    "            refer_index.append(pd.factorize(horse_df[col])[1])\n",
    "    except:\n",
    "        pass\n",
    "horse_df_sorted = horse_df.sort_values(by=['new_race no.'])\n",
    "horse_df_sorted.describe()\n",
    "cols = list(horse_df_sorted.columns)\n",
    "a , b = cols.index('new_Placing'), cols.index('declare_wt_diff')\n",
    "cols[b], cols[a] = cols[a], cols[b]\n",
    "horse_df_sorted = horse_df_sorted[cols]\n",
    "horse_wp_sorted = horse_df_sorted\n",
    "#change here\n",
    "#horse_df_sorted = horse_df_sorted.drop(columns = ['place odd', 'new_place odd'])\n",
    "horse_df_sorted = horse_df_sorted.drop(columns = ['place odd', 'new_place odd', 'new_Win Odds'])\n",
    "X, y = horse_df_sorted.iloc[:,:-1], horse_df_sorted.iloc[:,-1]\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle = False)\n",
    "# horse with place odd\n",
    "wpx , wpy = horse_wp_sorted.iloc[:,:-1], horse_wp_sorted.iloc[:,-1]\n",
    "no1, y1, n2, y2 = train_test_split(wpx, wpy, test_size=0.25, shuffle = False)\n",
    "data_dmatrix_train = xgb.DMatrix(data=X_train,label=y_train)\n",
    "data_dmatrix_test =  xgb.DMatrix(data=X_test,label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct a decision tree for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "no_na = no1.fillna(0)\n",
    "n2_cat = n2.astype('category')\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "model=DecisionTreeClassifier(max_depth=2,criterion='entropy')\n",
    "model.fit(no_na,n2_cat)\n",
    "plt.figure(figsize=(15,10))\n",
    "a = plot_tree(model, \n",
    "              feature_names=list(no_na.columns), \n",
    "              class_names=str(n2_cat.unique()), \n",
    "             filled=True, \n",
    "              rounded=True, \n",
    "              fontsize=8)\n",
    "pyplot.savefig('decision.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baysian optimization\n",
    "first define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = 10\n",
    "nc = len(np.unique(y))\n",
    "def xgb_evaluate(max_depth, gamma, colsample_bytree, n_estimators, eta, subsample):\n",
    "    params = {'objective': 'multi:softmax',\n",
    "              'eval_metric': 'mlogloss',\n",
    "              'max_depth': int(max_depth),\n",
    "              'subsample': subsample,\n",
    "              'eta': eta,\n",
    "              'gamma': gamma,\n",
    "              'nthread':8,\n",
    "              'num_class':nc,\n",
    "              'n_estimators':int(n_estimators),\n",
    "              'colsample_bytree': colsample_bytree}\n",
    "    # Used around 1000 boosting rounds in the full model\n",
    "    cv_result = xgb.cv(params, data_dmatrix_train, nfold=10)    \n",
    "    \n",
    "    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n",
    "    return -1.0 * cv_result['test-mlogloss-mean'].iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the range want to search and search the optimal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |    eta    |   gamma   | max_depth | n_esti... | subsample |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-2.732   \u001b[0m | \u001b[0m 0.4223  \u001b[0m | \u001b[0m 0.6477  \u001b[0m | \u001b[0m 1.649   \u001b[0m | \u001b[0m 8.357   \u001b[0m | \u001b[0m 147.6   \u001b[0m | \u001b[0m 0.7093  \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-2.605   \u001b[0m | \u001b[95m 0.7104  \u001b[0m | \u001b[95m 0.6182  \u001b[0m | \u001b[95m 2.991   \u001b[0m | \u001b[95m 6.053   \u001b[0m | \u001b[95m 238.1   \u001b[0m | \u001b[95m 0.7401  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-2.574   \u001b[0m | \u001b[95m 0.00391 \u001b[0m | \u001b[95m 0.735   \u001b[0m | \u001b[95m 4.668   \u001b[0m | \u001b[95m 7.983   \u001b[0m | \u001b[95m 726.4   \u001b[0m | \u001b[95m 0.364   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-5.386   \u001b[0m | \u001b[0m 0.2346  \u001b[0m | \u001b[0m 0.94    \u001b[0m | \u001b[0m 1.705   \u001b[0m | \u001b[0m 7.418   \u001b[0m | \u001b[0m 921.6   \u001b[0m | \u001b[0m 0.1049  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m-2.453   \u001b[0m | \u001b[95m 0.721   \u001b[0m | \u001b[95m 0.4301  \u001b[0m | \u001b[95m 0.9619  \u001b[0m | \u001b[95m 1.647   \u001b[0m | \u001b[95m 891.6   \u001b[0m | \u001b[95m 0.6294  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-2.836   \u001b[0m | \u001b[0m 0.7579  \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 0.5994  \u001b[0m | \u001b[0m 5.64    \u001b[0m | \u001b[0m 26.33   \u001b[0m | \u001b[0m 0.7023  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m-2.463   \u001b[0m | \u001b[0m 0.9349  \u001b[0m | \u001b[0m 0.2682  \u001b[0m | \u001b[0m 2.207   \u001b[0m | \u001b[0m 2.453   \u001b[0m | \u001b[0m 798.5   \u001b[0m | \u001b[0m 0.9755  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-3.214   \u001b[0m | \u001b[0m 0.3132  \u001b[0m | \u001b[0m 0.6957  \u001b[0m | \u001b[0m 4.04    \u001b[0m | \u001b[0m 8.57    \u001b[0m | \u001b[0m 519.6   \u001b[0m | \u001b[0m 0.01241 \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-2.607   \u001b[0m | \u001b[0m 0.2283  \u001b[0m | \u001b[0m 0.9648  \u001b[0m | \u001b[0m 1.675   \u001b[0m | \u001b[0m 2.937   \u001b[0m | \u001b[0m 463.6   \u001b[0m | \u001b[0m 0.2713  \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.523   \u001b[0m | \u001b[0m 0.4148  \u001b[0m | \u001b[0m 0.1483  \u001b[0m | \u001b[0m 4.51    \u001b[0m | \u001b[0m 8.39    \u001b[0m | \u001b[0m 595.4   \u001b[0m | \u001b[0m 0.4689  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-2.524   \u001b[0m | \u001b[0m 0.4525  \u001b[0m | \u001b[0m 0.1747  \u001b[0m | \u001b[0m 1.253   \u001b[0m | \u001b[0m 1.825   \u001b[0m | \u001b[0m 798.5   \u001b[0m | \u001b[0m 0.5195  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-2.627   \u001b[0m | \u001b[0m 0.945   \u001b[0m | \u001b[0m 0.2042  \u001b[0m | \u001b[0m 0.6299  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 877.6   \u001b[0m | \u001b[0m 0.864   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-2.922   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 812.2   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-2.515   \u001b[0m | \u001b[0m 0.9574  \u001b[0m | \u001b[0m 0.3215  \u001b[0m | \u001b[0m 4.014   \u001b[0m | \u001b[0m 8.742   \u001b[0m | \u001b[0m 782.8   \u001b[0m | \u001b[0m 0.5836  \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-2.519   \u001b[0m | \u001b[0m 0.1729  \u001b[0m | \u001b[0m 0.2481  \u001b[0m | \u001b[0m 4.901   \u001b[0m | \u001b[0m 9.001   \u001b[0m | \u001b[0m 621.7   \u001b[0m | \u001b[0m 0.5212  \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (0, 10), \n",
    "                                         'gamma': (0, 5),\n",
    "                                         'colsample_bytree': (0, 1),\n",
    "                                         'subsample': (0, 1),\n",
    "                                         'eta':(0.0001, 1),\n",
    "                                         'n_estimators': (5,1000),\n",
    "                                         })\n",
    "# Use the expected improvement acquisition function to handle negative numbers\n",
    "# Optimally needs quite a few more initiation points and number of iterations\n",
    "xgb_bo.maximize(init_points=10, n_iter=5, acq='ei')\n",
    "target = [xgb_bo.res[i]['target'] for i in range(0, len(xgb_bo.res))]\n",
    "for i in range(0, len(target)):\n",
    "    if xgb_bo.res[i]['target'] == max(target):\n",
    "        best_param = xgb_bo.res[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st\n",
    "#params = {'eta':   0.76848176873423635, 'max_depth': 2, 'subsample': 0.9, 'colsample_bytree': 0.733283853800647, 'objective': 'multi:softmax', 'seed': 4130, 'silent': 1, 'eval_metric':'mlogloss', 'nthread':8, 'num_class':nc, 'n_estimators': 246,'gamma': 0.12927266469325016, 'subsample': 0.7223685761352556}\n",
    "\n",
    "#2st\n",
    "params = {'eta':   0.4706790997792266, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.7907512352163658, 'objective': 'multi:softmax', 'seed': 39, 'silent': 1, 'eval_metric':'mlogloss', 'nthread':8, 'num_class':nc, 'n_estimators': 894,'gamma': 4.858986393127731, 'subsample': 0.929637360190657}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop for the different seed to find the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.1365\n",
      "Accuracy : 0.1408\n",
      "Accuracy : 0.1471\n",
      "Accuracy : 0.1381\n",
      "Accuracy : 0.1456\n",
      "Accuracy : 0.1408\n",
      "Accuracy : 0.133\n",
      "Accuracy : 0.1365\n",
      "Accuracy : 0.1404\n",
      "Accuracy : 0.1393\n"
     ]
    }
   ],
   "source": [
    "result_seed1 = []\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    params = {'eta':   0.4706790997792266, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.7907512352163658, 'objective': 'multi:softmax', 'seed':i, 'silent': 1, 'eval_metric':'mlogloss', 'nthread':8, 'num_class':nc, 'n_estimators': 894,'gamma': 4.858986393127731, 'subsample': 0.929637360190657}\n",
    "    xg_clf = xgb.train(params=params, dtrain=data_dmatrix_train)\n",
    "    preds = xg_clf.predict(data_dmatrix_test)\n",
    "    y_true= y_test\n",
    "    result_seed1.append([i, metrics.accuracy_score(y_true, preds)])\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, preds))\n",
    "    \n",
    "be = [result_seed[i][1] for i in range(0, len(result_seed))]\n",
    "max(be)\n",
    "np.mean(be)\n",
    "for i in range(0, len(be)):\n",
    "    if result_seed1[i][1] == max(be):\n",
    "        best_seed = result_seed1[i]\n",
    "#plot xgb_bo.res\n",
    "param_list = [xgb_bo.res[i]['params'] for i in range(0, len(xgb_bo.res))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the relationship between each parameters and the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame(target)\n",
    "bay_result = pd.DataFrame(param_list)\n",
    "bay_result = pd.concat([target_df,bay_result], axis = 1)\n",
    "bay_result = bay_result.sort_values(by=0, ascending=False)\n",
    "plt.figure(figsize = (20,8))\n",
    "#plt.plot(bay_result['colsample_bytree'], bay_result[0])\n",
    "#plt.plot(bay_result['eta'], bay_result[0])\n",
    "#plt.plot(bay_result[0], bay_result['gamma'])\n",
    "#plt.plot(bay_result[0], bay_result['max_depth'])\n",
    "plt.plot(bay_result[0], bay_result['n_estimators'])\n",
    "#plt.plot(bay_result['subsample'], bay_result[0])\n",
    "plt.legend(['number of estimators'], loc='upper left', fontsize = 18)\n",
    "pyplot.ylabel('mlogloss', fontsize=20)\n",
    "pyplot.show()\n",
    "pyplot.savefig('bay.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.head()\n",
    "#print((cv[\"test-mlogloss-mean\"]).tail(1))\n",
    "####################################\n",
    "# grid search\n",
    "# model = xgb.XGBClassifier()\n",
    "# n_estimators = [100, 200, 300, 400, 500]\n",
    "# learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "\n",
    "# param_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "# grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "# grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# # summarize results\n",
    "# print(); print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "# \t     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#      # plot results\n",
    "# pyplot.subplots(figsize=(12,12))\n",
    "# scores = numpy.array(means).reshape(len(learning_rate), len(n_estimators))\n",
    "\n",
    "# for i, value in enumerate(learning_rate):\n",
    "#     pyplot.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\n",
    "\n",
    "# pyplot.legend()\n",
    "# pyplot.xlabel('n_estimators')\n",
    "# pyplot.ylabel('Log Loss')\n",
    "# pyplot.show()\n",
    "# pyplot.savefig('n_estimators_vs_learning_rate.png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model with the best parameter, plot vip and confustion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.1393\n"
     ]
    }
   ],
   "source": [
    "xg_clf = xgb.train(params=params, dtrain=data_dmatrix_train)\n",
    "preds = xg_clf.predict(data_dmatrix_test)\n",
    "y_true= y_test\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, preds))\n",
    "#\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.show()\n",
    "\n",
    "xgb.plot_importance(xg_clf, max_num_features=10)\n",
    "plt.rcParams['figure.figsize'] = [150,50]\n",
    "plt.show()\n",
    "plt.savefig('vip.png')\n",
    "#confusion matrix\n",
    "plt.figure(figsize = (10,8))\n",
    "tab = pd.DataFrame(confusion_matrix(y_true, preds))\n",
    "tab = tab.drop(0, axis = 1)\n",
    "tab = tab.drop(0, axis = 0)\n",
    "sn.heatmap(tab, annot=True, annot_kws={\"size\": 7})\n",
    "\n",
    "pyplot.show()\n",
    "pyplot.savefig('confusion.png')\n",
    "#accuracy condition on 1\n",
    "tab[1][1]/(sum(tab[1])+sum(tab[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back testing for win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "odd_df = pd.concat([y1[['new_Win Odds','new_place odd']], y_true], axis = 1)\n",
    "win_df = odd_df[np.array(y_true) == preds]\n",
    "win_df = win_df[win_df['new_Placing'] == 1]\n",
    "#cumsum\n",
    "lose_df = np.repeat(-1,(len(preds[preds == 1]) - len(win_df['new_Win Odds'])))\n",
    "\n",
    "combine_df = pd.concat([win_df['new_Win Odds'], pd.DataFrame(lose_df)])\n",
    "combine_df = combine_df.sample(frac=1).reset_index(drop=True)\n",
    "cum_win = np.cumsum(combine_df)*10\n",
    "cum_win.plot()\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "plt.legend(['return rate = 78.74%'], loc='upper left', fontsize=20)\n",
    "pyplot.title(\"Win return\", fontsize=20)\n",
    "pyplot.xlabel('trail', fontsize=20)\n",
    "pyplot.ylabel('amount of bet', fontsize=20)\n",
    "pyplot.show()\n",
    "pyplot.savefig('winodds1.png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back testing for place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "con1 = y_true<4\n",
    "con2 = abs(np.array(y_true) - preds) <=3\n",
    "con3 = y_true>0\n",
    "preds_place = odd_df\n",
    "preds_place_3 = preds_place[preds <=3]\n",
    "place_return_df = preds_place_3['new_place odd']\n",
    "place_return_df = place_return_df.fillna(-1)\n",
    "place_return_df = np.array(place_return_df)\n",
    "cum_place = np.cumsum(pd.DataFrame(place_return_df))*10\n",
    "#cumsum plot\n",
    "cum_place.plot()\n",
    "\n",
    "pyplot.legend()\n",
    "pyplot.title(\"Place return\", fontsize=20)\n",
    "pyplot.xlabel('trail', fontsize=20)\n",
    "pyplot.ylabel('amount of bet', fontsize=20)\n",
    "plt.legend(['return rate = 31.34%'], loc='upper left', fontsize=20)\n",
    "pyplot.show()\n",
    "pyplot.savefig('place1.png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot xg boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "# load data\n",
    "# fit model no training data\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:\\Users\\admin\\anaconda3\\pkgs\\graphviz-2.38-hfd603c8_2\\Library\\bin\\graphviz'\n",
    "# plot single tree\n",
    "fig, ax = plt.subplots(figsize=(60, 60))\n",
    "xgb.plot_tree(xg_clf,num_trees=2, ax=ax)\n",
    "fig.set_size_inches(200, 200)\n",
    "plt.show()\n",
    "plt.savefig('xgb.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
